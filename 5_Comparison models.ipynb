{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import itertools\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras_tuner import RandomSearch\n",
    "from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score, make_scorer, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score, RepeatedKFold, RandomizedSearchCV, KFold, StratifiedKFold\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from kneed import KneeLocator\n",
    "from sklearn.cluster import KMeans\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from scipy.stats import norm\n",
    "from sklearn.svm import SVR\n",
    "from scipy.stats import shapiro\n",
    "import shap\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data recurring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "with open(\"Data final//data_recurring_imputed.pkl\", \"rb\") as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into dependent and independent variables\n",
    "X = data.drop([\"Total_revenue\"], axis = \"columns\")\n",
    "y = data[\"Total_revenue\"]\n",
    "# Split data into training and test sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.25, random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop insignificant and multicollinear variables \n",
    "# This is the result of iteratively running the model and removing insignificant and multicollinear variables\n",
    "train_X = train_X.drop([\"Quarter_2\", \"Number_account\", \"Exchange rate\", \"Consumption of durables\", \"Unemployment rate\", \"Interest rate\", \"GDP growth\", \"Quarter_4\", \"Limit_AMT_max\", \"Age\", \"Consumer confidence index\", \"Limit_AMT_min\", \"Inflation\"], axis = \"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a constant\n",
    "train_X = sm.add_constant(train_X)\n",
    "# Train a Linear Regression model\n",
    "model_linear = sm.OLS(train_y, train_X).fit(cov_type='HC3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate VIF\n",
    "vif = pd.DataFrame([variance_inflation_factor(train_X.drop([\"const\"], axis = \"columns\").values, i) for i in range(train_X.drop([\"const\"], axis = \"columns\").shape[1])], index=train_X.drop([\"const\"], axis = \"columns\").columns, columns=['VIF'])\n",
    "print('Variance Inflation Factors:')\n",
    "print(vif.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check significant variables and coefficients\n",
    "print(model_linear.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract p-values\n",
    "p_values = model_linear.pvalues[1:] # exclude the intercept\n",
    "\n",
    "# Apply Bonferroni correction\n",
    "alpha = 0.05\n",
    "reject, adjusted_p_values, _, _ = multipletests(p_values, alpha=alpha, method='bonferroni')\n",
    "\n",
    "# Print the results\n",
    "print('P-values:', p_values)\n",
    "print('Adjusted p-values:', adjusted_p_values)\n",
    "print('Rejected hypotheses:', reject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Breusch-Pagan test for heteroskedasticity\n",
    "bp_test = het_breuschpagan(model_linear.resid, model_linear.model.exog)\n",
    "print('Breusch-Pagan test p-value:', bp_test[1]) # heteroskedastic\n",
    "\n",
    "# Run Durbin-Watson test for autocorrelation\n",
    "dw_test = durbin_watson(model_linear.resid)\n",
    "print('Durbin-Watson test statistic:', dw_test)\n",
    "print('Durbin-Watson test p-value:', 2 * norm.cdf(-abs(dw_test - 2))) # no autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data set\n",
    "test_X = test_X.drop([\"Quarter_2\", \"Number_account\", \"Exchange rate\", \"Consumption of durables\", \"Unemployment rate\", \"Interest rate\", \"GDP growth\", \"Quarter_4\", \"Limit_AMT_max\", \"Age\", \"Consumer confidence index\", \"Limit_AMT_min\", \"Inflation\"], axis = \"columns\")\n",
    "test_X = sm.add_constant(test_X)\n",
    "# Calculate perdictions for test data set\n",
    "pred = model_linear.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MAE\n",
    "print(f\"MAE: {mean_absolute_error(test_y, pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE \n",
    "print(f\"MSE: {mean_squared_error(test_y, pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE\n",
    "print(f\"RMSE: {mean_squared_error(test_y, pred, squared=False):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "with open(\"Data final//data_new_imputed.pkl\", \"rb\") as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into dependent and independent variables\n",
    "X = data.drop([\"Total_revenue\"], axis = \"columns\")\n",
    "y = data[\"Total_revenue\"]\n",
    "# Split data into training and test sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.25, random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop insignificant and multicollinear variables \n",
    "# This is the result of iteratively running the model and removing insignificant and multicollinear variables\n",
    "train_X = train_X.drop([\"GDP growth\", \"Consumption of durables\", \"Inflation\", \"Quarter_2\", \"Exchange rate\", \"Unemployment rate\", \"Quarter_3\", \"Consumer confidence index\", \"Interest rate\", \"Age\"], axis = \"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a constant\n",
    "train_X = sm.add_constant(train_X)\n",
    "# Train a Linear Regression model\n",
    "model_linear = sm.OLS(train_y, train_X).fit(cov_type='HC3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate VIF\n",
    "vif = pd.DataFrame([variance_inflation_factor(train_X.drop([\"const\"], axis = \"columns\").values, i) for i in range(train_X.drop([\"const\"], axis = \"columns\").shape[1])], index=train_X.drop([\"const\"], axis = \"columns\").columns, columns=['VIF'])\n",
    "print('Variance Inflation Factors:')\n",
    "print(vif.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check significant variables and coefficients\n",
    "print(model_linear.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract p-values\n",
    "p_values = model_linear.pvalues[1:] # exclude the intercept\n",
    "\n",
    "# Apply Bonferroni correction\n",
    "alpha = 0.05\n",
    "reject, adjusted_p_values, _, _ = multipletests(p_values, alpha=alpha, method='bonferroni')\n",
    "\n",
    "# Print the results\n",
    "print('P-values:', p_values)\n",
    "print('Adjusted p-values:', adjusted_p_values)\n",
    "print('Rejected hypotheses:', reject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Breusch-Pagan test for heteroskedasticity\n",
    "bp_test = het_breuschpagan(model_linear.resid, model_linear.model.exog)\n",
    "print('Breusch-Pagan test p-value:', bp_test[1]) # heteroskedastic\n",
    "\n",
    "# Run Durbin-Watson test for autocorrelation\n",
    "dw_test = durbin_watson(model_linear.resid)\n",
    "print('Durbin-Watson test statistic:', dw_test)\n",
    "print('Durbin-Watson test p-value:', 2 * norm.cdf(-abs(dw_test - 2))) # no autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data set\n",
    "test_X = test_X.drop([\"GDP growth\", \"Consumption of durables\", \"Inflation\", \"Quarter_2\", \"Exchange rate\", \"Unemployment rate\", \"Quarter_3\", \"Consumer confidence index\", \"Interest rate\", \"Age\"], axis = \"columns\")\n",
    "test_X = sm.add_constant(test_X)\n",
    "# Calculate perdictions for test data set\n",
    "pred = model_linear.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MAE\n",
    "print(f\"MAE: {mean_absolute_error(test_y, pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE \n",
    "print(f\"MSE: {mean_squared_error(test_y, pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE\n",
    "print(f\"RMSE: {mean_squared_error(test_y, pred, squared=False):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random search with neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data recurring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data\n",
    "with open('Data final//data_recurring_imputed.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 1000000 rows to perform a random search\n",
    "index_sample = random.sample(range(0, len(data)), 1000000)\n",
    "data = data.iloc[index_sample,]\n",
    "# Divide data into dependent and independent variables\n",
    "X = data.drop([\"Total_revenue\"], axis = \"columns\")\n",
    "y = data[\"Total_revenue\"]\n",
    "# Divide data into train and test data sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.25, random_state = 3)\n",
    "# Standardise data \n",
    "scaler = StandardScaler()\n",
    "train_X = scaler.fit_transform(train_X)\n",
    "test_X = scaler.transform(test_X)\n",
    "# Choose ten learning rates ranging from 0.000001 to 0.001\n",
    "learning_rates = list(np.logspace(-6, -3, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the architecture of a neural network \n",
    "def create_model_neural_net(hp):\n",
    "    model_net = Sequential()\n",
    "    # Add a hidden layer with 2000 hidden units and a ReLu activation function\n",
    "    model_net.add(Dense(2000, input_shape=(train_X.shape[1],), activation='relu'))\n",
    "    # Add a dropout layer with a 50% dropout rate\n",
    "    model_net.add(Dropout(0.5))\n",
    "    # Add a hidden layer with 1000 hidden units and a ReLu activation function\n",
    "    model_net.add(Dense(1000, activation='relu'))\n",
    "    # Add a dropout layer with a 30% dropout rate\n",
    "    model_net.add(Dropout(0.3))\n",
    "    # Add a hidden layer with 500 hidden units and a ReLu activation function\n",
    "    model_net.add(Dense(500, activation='relu'))\n",
    "    # Add a dropout layer with a 30% dropout rate\n",
    "    model_net.add(Dropout(0.3))\n",
    "     # Add a hidden layer with 250 hidden units and a ReLu activation function\n",
    "    model_net.add(Dense(250, activation='relu'))\n",
    "    # Add a dropout layer with a 20% dropout rate\n",
    "    model_net.add(Dropout(0.2))\n",
    "     # Add an output layer with 1 hidden unit and a linear activation function\n",
    "    model_net.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    # Choose a loss function, evaluation metrics and optimizers\n",
    "    model_net.compile(optimizer=hp.Choice('optimizer', values=['adam', 'rmsprop']), \n",
    "                  loss='mse', \n",
    "                  metrics=['mae'])\n",
    "\n",
    "    # Set possible learning rates that will be checked\n",
    "    model_net.optimizer.learning_rate = hp.Choice('learning_rate', values=[1e-06, 2.1544346900318822e-06, 4.641588833612782e-06, 1e-05, 2.1544346900318823e-05, 4.641588833612772e-05, 0.0001, 0.00021544346900318823, 0.00046415888336127773, 0.001])\n",
    "    return model_net\n",
    "\n",
    "# Define the search space of hyperparameters\n",
    "tuner_hp = HyperParameters()\n",
    "# Set possible optimizers that will be checked\n",
    "tuner_hp.Choice('optimizer', values=['adam', \"rmsprop\"])\n",
    "# Set possible learning rates that will be checked\n",
    "tuner_hp.Choice('learning_rate', values= [1e-06, 2.1544346900318822e-06, 4.641588833612782e-06, 1e-05, 2.1544346900318823e-05, 4.641588833612772e-05, 0.0001, 0.00021544346900318823, 0.00046415888336127773, 0.001])\n",
    "\n",
    "# Create the tuner object\n",
    "tuner = RandomSearch(\n",
    "    # Assign a neural network \n",
    "    create_model_neural_net,\n",
    "    # Assign available parameters\n",
    "    hyperparameters=tuner_hp,\n",
    "    # Choose validation loss as the optimisation objective\n",
    "    objective='val_loss',\n",
    "    # Choose the maximum number of trials to check\n",
    "    max_trials=10,\n",
    "    overwrite = True,\n",
    "    project_name='my_project')\n",
    "\n",
    "# Set an early stopping procedure\n",
    "early_stopping = EarlyStopping(\n",
    "    # Choose validation loss as the monitored metric\n",
    "    monitor='val_loss', \n",
    "    # Set the patience to 5. Patience is the number of epochs with no improvement after which training will be stopped\n",
    "    patience = 5, \n",
    "    # Set the mode to 'min', which means that training will stop when the quantity monitored has stopped decreasing\n",
    "    mode = \"min\", \n",
    "    # Leave the weights which generated the lowest validation loss\n",
    "    restore_best_weights = True,\n",
    "    # Set the minimum change in the monitored quantity to qualify as an improvement to 0.01\n",
    "    min_delta=0.01\n",
    "    )\n",
    "\n",
    "# Run the search\n",
    "tuner.search(train_X, \n",
    "             train_y,\n",
    "             # Run the search for 30 epochs\n",
    "             epochs=30, \n",
    "             # Split the data such that 80% is allocated for training and 20% is reserved for validation\n",
    "             validation_split = 0.2,\n",
    "             callbacks=[early_stopping]\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the best model\n",
    "model_neural_net_best=tuner.get_best_models()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the parameters of the best model\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "learning_rate = best_hyperparameters.get('learning_rate')\n",
    "optimizer = best_hyperparameters.get('optimizer')\n",
    "print(f\"Best learning rate: {learning_rate}\")\n",
    "print(f\"Best optimizer: {optimizer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model based on MAE\n",
    "model_neural_net_best.evaluate(test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data\n",
    "with open('Data final//data_new_imputed.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into dependent and independent variables\n",
    "X = data.drop([\"Total_revenue\"], axis = \"columns\")\n",
    "y = data[\"Total_revenue\"]\n",
    "# Divide data into train and test data sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.25, random_state = 3)\n",
    "# Standardise data \n",
    "scaler = StandardScaler()\n",
    "train_X = scaler.fit_transform(train_X)\n",
    "test_X = scaler.transform(test_X)\n",
    "# Choose ten learning rates ranging from 0.000001 to 0.001\n",
    "learning_rates = list(np.logspace(-6, -3, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the architecture of a neural network \n",
    "def create_model_neural_net(hp):\n",
    "    model_net = Sequential()\n",
    "    # Add a hidden layer with 2000 hidden units and a ReLu activation function\n",
    "    model_net.add(Dense(2000, input_shape=(train_X.shape[1],), activation='relu'))\n",
    "    # Add a dropout layer with a 50% dropout rate\n",
    "    model_net.add(Dropout(0.5))\n",
    "    # Add a hidden layer with 1000 hidden units and a ReLu activation function\n",
    "    model_net.add(Dense(1000, activation='relu'))\n",
    "    # Add a dropout layer with a 30% dropout rate\n",
    "    model_net.add(Dropout(0.3))\n",
    "    # Add a hidden layer with 500 hidden units and a ReLu activation function\n",
    "    model_net.add(Dense(500, activation='relu'))\n",
    "    # Add a dropout layer with a 30% dropout rate\n",
    "    model_net.add(Dropout(0.3))\n",
    "     # Add a hidden layer with 250 hidden units and a ReLu activation function\n",
    "    model_net.add(Dense(250, activation='relu'))\n",
    "    # Add a dropout layer with a 20% dropout rate\n",
    "    model_net.add(Dropout(0.2))\n",
    "     # Add an output layer with 1 hidden unit and a linear activation function\n",
    "    model_net.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    # Choose a loss function, evaluation metrics and optimizers\n",
    "    model_net.compile(optimizer=hp.Choice('optimizer', values=['adam', 'rmsprop']), \n",
    "                  loss='mse', \n",
    "                  metrics=['mae'])\n",
    "\n",
    "    # Set possible learning rates that will be checked\n",
    "    model_net.optimizer.learning_rate = hp.Choice('learning_rate', values=[1e-06, 2.1544346900318822e-06, 4.641588833612782e-06, 1e-05, 2.1544346900318823e-05, 4.641588833612772e-05, 0.0001, 0.00021544346900318823, 0.00046415888336127773, 0.001])\n",
    "    return model_net\n",
    "\n",
    "# Define the search space of hyperparameters\n",
    "tuner_hp = HyperParameters()\n",
    "# Set possible optimizers that will be checked\n",
    "tuner_hp.Choice('optimizer', values=['adam', \"rmsprop\"])\n",
    "# Set possible learning rates that will be checked\n",
    "tuner_hp.Choice('learning_rate', values= [1e-06, 2.1544346900318822e-06, 4.641588833612782e-06, 1e-05, 2.1544346900318823e-05, 4.641588833612772e-05, 0.0001, 0.00021544346900318823, 0.00046415888336127773, 0.001])\n",
    "\n",
    "# Create the tuner object\n",
    "tuner = RandomSearch(\n",
    "    # Assign a neural network \n",
    "    create_model_neural_net,\n",
    "    # Assign available parameters\n",
    "    hyperparameters=tuner_hp,\n",
    "    # Choose validation loss as the optimisation objective\n",
    "    objective='val_loss',\n",
    "    # Choose the maximum number of trials to check\n",
    "    max_trials=10,\n",
    "    overwrite = True,\n",
    "    project_name='my_project')\n",
    "\n",
    "# Set an early stopping procedure\n",
    "early_stopping = EarlyStopping(\n",
    "    # Choose validation loss as the monitored metric\n",
    "    monitor='val_loss', \n",
    "    # Set the patience to 5. Patience is the number of epochs with no improvement after which training will be stopped\n",
    "    patience = 5, \n",
    "    # Set the mode to 'min', which means that training will stop when the quantity monitored has stopped decreasing\n",
    "    mode = \"min\", \n",
    "    # Leave the weights which generated the lowest validation loss\n",
    "    restore_best_weights = True,\n",
    "    # Set the minimum change in the monitored quantity to qualify as an improvement to 0.01\n",
    "    min_delta=0.01\n",
    "    )\n",
    "\n",
    "# Run the search\n",
    "tuner.search(train_X, \n",
    "             train_y,\n",
    "             # Run the search for 30 epochs\n",
    "             epochs=30, \n",
    "             # Split the data such that 80% is allocated for training and 20% is reserved for validation\n",
    "             validation_split = 0.2,\n",
    "             callbacks=[early_stopping]\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the best model\n",
    "model_neural_net_best=tuner.get_best_models()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the parameters of the best model\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "learning_rate = best_hyperparameters.get('learning_rate')\n",
    "optimizer = best_hyperparameters.get('optimizer')\n",
    "print(f\"Best learning rate: {learning_rate}\")\n",
    "print(f\"Best optimizer: {optimizer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model based on MAE\n",
    "model_neural_net_best.evaluate(test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model neural nets with the best parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data recurring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data\n",
    "with open('Data final//data_recurring_imputed.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 1000000 observations from data\n",
    "index_sample = random.sample(range(0, len(data)), 1000000)\n",
    "data = data.iloc[index_sample,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into dependent and independent variables\n",
    "X = data.drop([\"Total_revenue\"], axis = \"columns\")\n",
    "y = data[\"Total_revenue\"]\n",
    "# Divide data into train and test data sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.25, random_state = 3)\n",
    "# Standardise data \n",
    "scaler = StandardScaler()\n",
    "train_X = scaler.fit_transform(train_X)\n",
    "test_X = scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the architecture of a neural network \n",
    "model_net = Sequential()\n",
    "# Add a hidden layer with 2000 hidden units and a ReLu activation function\n",
    "model_net.add(Dense(2000, input_shape=(train_X.shape[1],), activation='relu'))\n",
    "# Add a dropout layer with a 50% dropout rate\n",
    "model_net.add(Dropout(0.5))\n",
    "# Add a hidden layer with 1000 hidden units and a ReLu activation function\n",
    "model_net.add(Dense(1000, activation='relu'))\n",
    "# Add a dropout layer with a 30% dropout rate\n",
    "model_net.add(Dropout(0.3))\n",
    "# Add a hidden layer with 500 hidden units and a ReLu activation function\n",
    "model_net.add(Dense(500, activation='relu'))\n",
    "# Add a dropout layer with a 30% dropout rate\n",
    "model_net.add(Dropout(0.3))\n",
    "# Add a hidden layer with 250 hidden units and a ReLu activation function\n",
    "model_net.add(Dense(250, activation='relu'))\n",
    "# Add a dropout layer with a 20% dropout rate\n",
    "model_net.add(Dropout(0.2))\n",
    "# Add an output layer with 1 hidden unit and a linear activation function\n",
    "model_net.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the neural network with an Adam optimizer and a learning rate of 0.0001\n",
    "model_net.compile(loss='mse', optimizer= Adam(learning_rate=0.0001), metrics=['mae'])\n",
    "\n",
    "# Define the checkpoint path and a metric to monitor\n",
    "checkpoint_path = \"best_model.h5\"\n",
    "monitor_metric = 'val_loss'\n",
    "\n",
    "# Create the ModelCheckpoint callback\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, monitor=monitor_metric, save_best_only=True, mode='max')\n",
    "\n",
    "# Set an early stopping procedure\n",
    "es = EarlyStopping(\n",
    "    # Choose validation loss as the monitored metric\n",
    "    monitor='val_loss',\n",
    "    # Set the mode to 'min', which means that training will stop when the quantity monitored has stopped decreasing\n",
    "    mode='min',\n",
    "    # Set the patience to 5. Patience is the number of epochs with no improvement after which training will be stopped.\n",
    "    patience=5,\n",
    "    # Leave the weights which generated the lowest validation loss\n",
    "    restore_best_weights = True)\n",
    "\n",
    "# Fit the model \n",
    "history  = model_net.fit(train_X, train_y, \n",
    "                    # Run the search for 30 epochs\n",
    "                    epochs=30, \n",
    "                    # Choose the batch sie\n",
    "                    batch_size=32,\n",
    "                    # Split the data such that 80% is allocated for training and 20% is reserved for validation.\n",
    "                    validation_split = 0.2,\n",
    "                    callbacks=[es, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model  \n",
    "model_net.evaluate(test_X, test_y)\n",
    "# Calculate predictions\n",
    "pred = model_net.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE \n",
    "mean_squared_error(test_y, pred)\n",
    "# Calculate RMSE\n",
    "mean_squared_error(test_y, pred, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data\n",
    "with open('Data final//data_new_imputed.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into dependent and independent variables\n",
    "X = data.drop([\"Total_revenue\"], axis = \"columns\")\n",
    "y = data[\"Total_revenue\"]\n",
    "# Divide data into train and test data sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.25, random_state = 3)\n",
    "# Standardise data \n",
    "scaler = StandardScaler()\n",
    "train_X = scaler.fit_transform(train_X)\n",
    "test_X = scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the architecture of a neural network \n",
    "model_net = Sequential()\n",
    "# Add a hidden layer with 2000 hidden units and a ReLu activation function\n",
    "model_net.add(Dense(2000, input_shape=(train_X.shape[1],), activation='relu'))\n",
    "# Add a dropout layer with a 50% dropout rate\n",
    "model_net.add(Dropout(0.5))\n",
    "# Add a hidden layer with 1000 hidden units and a ReLu activation function\n",
    "model_net.add(Dense(1000, activation='relu'))\n",
    "# Add a dropout layer with a 30% dropout rate\n",
    "model_net.add(Dropout(0.3))\n",
    "# Add a hidden layer with 500 hidden units and a ReLu activation function\n",
    "model_net.add(Dense(500, activation='relu'))\n",
    "# Add a dropout layer with a 30% dropout rate\n",
    "model_net.add(Dropout(0.3))\n",
    "# Add a hidden layer with 250 hidden units and a ReLu activation function\n",
    "model_net.add(Dense(250, activation='relu'))\n",
    "# Add a dropout layer with a 20% dropout rate\n",
    "model_net.add(Dropout(0.2))\n",
    "# Add an output layer with 1 hidden unit and a linear activation function\n",
    "model_net.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the neural network with an Adam optimizer and a learning rate of 0.0001\n",
    "model_net.compile(loss='mse', optimizer= Adam(learning_rate=0.00046), metrics=['mae'])\n",
    "\n",
    "# Define the checkpoint path and a metric to monitor\n",
    "checkpoint_path = \"best_model.h5\"\n",
    "monitor_metric = 'val_loss'\n",
    "\n",
    "# Create the ModelCheckpoint callback\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, monitor=monitor_metric, save_best_only=True, mode='max')\n",
    "\n",
    "# Set an early stopping procedure\n",
    "es = EarlyStopping(\n",
    "    # Choose validation loss as the monitored metric\n",
    "    monitor='val_loss',\n",
    "    # Set the mode to 'min', which means that training will stop when the quantity monitored has stopped decreasing\n",
    "    mode='min',\n",
    "    # Set the patience to 5. Patience is the number of epochs with no improvement after which training will be stopped.\n",
    "    patience=5,\n",
    "    # Leave the weights which generated the lowest validation loss\n",
    "    restore_best_weights = True)\n",
    "\n",
    "# Fit the model \n",
    "history  = model_net.fit(train_X, train_y, \n",
    "                    # Run the search for 30 epochs\n",
    "                    epochs=30, \n",
    "                    # Choose the batch sie\n",
    "                    batch_size=32,\n",
    "                    # Split the data such that 80% is allocated for training and 20% is reserved for validation.\n",
    "                    validation_split = 0.2,\n",
    "                    callbacks=[es, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model  \n",
    "model_net.evaluate(test_X, test_y)\n",
    "# Calculate predictions\n",
    "pred = model_net.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE \n",
    "mean_squared_error(test_y, pred)\n",
    "# Calculate RMSE\n",
    "mean_squared_error(test_y, pred, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random search with XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data recurring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data\n",
    "with open('Data final//data_recurring_imputed.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 1000000 rows to perform a random search\n",
    "index_sample = random.sample(range(0, len(data)), 1000000)\n",
    "data = data.iloc[index_sample,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into dependent and independent variables\n",
    "X = data.drop([\"Total_revenue\"], axis = \"columns\")\n",
    "y = data[\"Total_revenue\"]\n",
    "# Divide data into train and test data sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.25, random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters that will be checked for a random search\n",
    "params = {\n",
    "    'n_estimators':[70, 100, 200],\n",
    "    'min_child_weight':[1, 2, 3], \n",
    "    'gamma':[i/10.0 for i in range(0,3)],  \n",
    "    'subsample':[i/10.0 for i in range(6,11)],\n",
    "    'colsample_bytree':[i/10.0 for i in range(6,11)], \n",
    "    'max_depth': [2,3,4,6,7],\n",
    "    'objective': ['reg:squarederror'],\n",
    "    'booster': ['gbtree', 'gblinear'],\n",
    "    'eta': [i/10.0 for i in range(3,6)],\n",
    "}\n",
    "\n",
    "# Define a model\n",
    "reg = XGBRegressor(nthread=-1)\n",
    "\n",
    "# Set the number of iterations for a random search\n",
    "n_iter_search = 50\n",
    "# Define a random search\n",
    "random_search = RandomizedSearchCV(reg, param_distributions=params,\n",
    "                                   n_iter=n_iter_search, cv=5, scoring='neg_mean_absolute_error')\n",
    "\n",
    "\n",
    "# Perform a random search\n",
    "random_search.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the best model\n",
    "best_regressor_xgb = random_search.best_estimator_\n",
    "# Extract the best parameters\n",
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "pred = best_regressor_xgb.predict(test_X)\n",
    "# Calculate MAE\n",
    "mean_absolute_error(test_y, pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE\n",
    "mean_squared_error(test_y, pred, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data\n",
    "with open('Data final//data_new_imputed.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into dependent and independent variables\n",
    "X = data.drop([\"Total_revenue\"], axis = \"columns\")\n",
    "y = data[\"Total_revenue\"]\n",
    "# Divide data into train and test data sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.25, random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters that will be checked for a random search\n",
    "params = {\n",
    "    'n_estimators':[70, 100, 200],\n",
    "    'min_child_weight':[1, 2, 3], \n",
    "    'gamma':[i/10.0 for i in range(0,3)],  \n",
    "    'subsample':[i/10.0 for i in range(6,11)],\n",
    "    'colsample_bytree':[i/10.0 for i in range(6,11)], \n",
    "    'max_depth': [2,3,4,6,7],\n",
    "    'objective': ['reg:squarederror'],\n",
    "    'booster': ['gbtree', 'gblinear'],\n",
    "    'eta': [i/10.0 for i in range(3,6)],\n",
    "}\n",
    "\n",
    "# Define a model\n",
    "reg = XGBRegressor(nthread=-1)\n",
    "\n",
    "# Set the number of iterations for a random search\n",
    "n_iter_search = 50\n",
    "# Define a random search\n",
    "random_search = RandomizedSearchCV(reg, param_distributions=params,\n",
    "                                   n_iter=n_iter_search, cv=5, scoring='neg_mean_absolute_error')\n",
    "\n",
    "\n",
    "# Perform a random search\n",
    "random_search.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the best model\n",
    "best_regressor_xgb = random_search.best_estimator_\n",
    "# Extract the best parameters\n",
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "pred = best_regressor_xgb.predict(test_X)\n",
    "# Calculate MAE\n",
    "mean_absolute_error(test_y, pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE\n",
    "mean_squared_error(test_y, pred, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model XGBoost with the best parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data recurring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data\n",
    "with open('Data final//data_recurring_imputed.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into dependent and independent variables\n",
    "X = data.drop([\"Total_revenue\"], axis = \"columns\")\n",
    "y = data[\"Total_revenue\"]\n",
    "# Divide data into train and test data sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.25, random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model with the best parameters \n",
    "model_xgb = XGBRegressor(subsample = 0.9, objective = 'reg:squarederror', n_estimators = 200, min_child_weight = 2, \n",
    "                    max_depth = 6, gamma = 0.1, eta = 0.3, colsample_bytree = 0.8, booster = 'gbtree')\n",
    "  \n",
    "# Fit a model\n",
    "model_xgb.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate predictions\n",
    "pred = model_xgb.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MAE\n",
    "mean_absolute_error(test_y, pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE\n",
    "mean_squared_error(test_y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE\n",
    "mean_squared_error(test_y, pred, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable importance with Shapley values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 20 000 observations to calculate Shapley values\n",
    "index_sample = random.sample(range(0, len(test_X)), 20000)\n",
    "shap_X = test_X.iloc[index_sample]\n",
    "\n",
    "# Get Shapley values\n",
    "shap_values = shap.TreeExplainer(model_xgb).shap_values(shap_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the variables by importance (box plot)\n",
    "fig = plt.figure()\n",
    "shap.summary_plot(shap_values, shap_X, plot_type=\"bar\", show = False, color = \"#9797ff\")\n",
    "plt.xlabel(\"Shapley value\")\n",
    "plt.xlim([0,300])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the variables with detail of Shapley values (beeswarm plot)\n",
    "fig = plt.figure()\n",
    "shap.summary_plot(shap_values, shap_X, show = False, cmap = \"coolwarm\")\n",
    "plt.xlabel(\"Shapley value\")\n",
    "plt.xlim([-2000,5500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data\n",
    "with open('Data final//data_new_imputed.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into dependent and independent variables\n",
    "X = data.drop([\"Total_revenue\"], axis = \"columns\")\n",
    "y = data[\"Total_revenue\"]\n",
    "# Divide data into train and test data sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.25, random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model\n",
    "model_xgb = XGBRegressor(subsample = 1, objective = 'reg:squarederror', n_estimators = 70, min_child_weight = 3, \n",
    "                    max_depth = 6, gamma = 0.2, eta = 0.3, colsample_bytree = 0.8, booster = 'gbtree')\n",
    "  \n",
    "# Fit a model\n",
    "model_xgb.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate predictions\n",
    "pred = model_xgb.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MAE\n",
    "mean_absolute_error(test_y, pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE\n",
    "mean_squared_error(test_y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE\n",
    "mean_squared_error(test_y, pred, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable importance with Shapley values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 20 000 observations to calculate Shapley values\n",
    "index_sample = random.sample(range(0, len(test_X)), 20000)\n",
    "shap_X = test_X.iloc[index_sample]\n",
    "\n",
    "# Get Shapley values\n",
    "shap_values = shap.TreeExplainer(model_xgb).shap_values(shap_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the variables by importance (box plot)\n",
    "fig = plt.figure()\n",
    "shap.summary_plot(shap_values, shap_X, plot_type=\"bar\", show = False, color = \"#9797ff\")\n",
    "plt.xlabel(\"Shapley value\")\n",
    "plt.xlim([0,60])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the variables with detail of Shapley values (beeswarm plot)\n",
    "fig = plt.figure()\n",
    "shap.summary_plot(shap_values, shap_X, show = False, cmap = \"coolwarm\")\n",
    "plt.xlabel(\"Shapley value\")\n",
    "plt.xlim([-1500,3000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random search with Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data recurring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data\n",
    "with open('Data final//data_recurring_imputed.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 50000 observations from data\n",
    "index_sample = random.sample(range(0, len(data)), 50000)\n",
    "data = data.iloc[index_sample,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into dependent and independent variables\n",
    "y = data[\"Total_revenue\"]\n",
    "X = data.drop(\"Total_revenue\", axis = \"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define search range for random search for...\n",
    "# number of decision trees to use\n",
    "n_estimators = [40, 50, 60, 70, 80] \n",
    "# loss function\n",
    "criterion = [\"squared_error\", \"absolute_error\"]\n",
    "# maximum number of features to consider at every split\n",
    "max_features = [\"log2\", \"sqrt\"]\n",
    "# maximum number of levels in each tree\n",
    "max_depth = [3, 5, 10, 15, 20]\n",
    "# minimum number of observations to split a node \n",
    "min_samples_split = [0.0005, 0.001, 0.0025, 0.005] \n",
    "# use bootstrap samples\n",
    "bootstrap = [True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random grid\n",
    "random_grid = {\"n_estimators\": n_estimators, \"criterion\": criterion, \"max_features\": max_features, \"max_depth\": max_depth,\n",
    "               \"min_samples_split\": min_samples_split, \"bootstrap\": bootstrap}\n",
    "# Define estimator (random forest)\n",
    "rf = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random search (tries 50 different random combinations out of 800 possible ones)\n",
    "rf_random_search = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,\n",
    "                                      n_iter = 50, cv = 2, verbose = 2, n_jobs = -1)\n",
    "rf_random_search_fit = rf_random_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the best parameters found in the random search \n",
    "rf_random_search_best = rf_random_search.best_params_ \n",
    "print(rf_random_search_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data\n",
    "with open('Data final//data_new_imputed.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 50000 rows to perform a random search\n",
    "index_sample = random.sample(range(0, len(data)), 50000)\n",
    "data = data.iloc[index_sample,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into dependent and independent variables\n",
    "y = data[\"Total_revenue\"]\n",
    "X = data.drop(\"Total_revenue\", axis = \"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define search range for random search for...\n",
    "# number of decision trees to use\n",
    "n_estimators = [40, 50, 60, 70, 80] \n",
    "# loss function\n",
    "criterion = [\"squared_error\", \"absolute_error\"]\n",
    "# maximum number of features to consider at every split\n",
    "max_features = [\"log2\", \"sqrt\"]\n",
    "# maximum number of levels in each tree\n",
    "max_depth = [3, 5, 10, 15, 20]\n",
    "# minimum number of observations to split a node \n",
    "min_samples_split = [0.0005, 0.001, 0.0025, 0.005] \n",
    "# use bootstrap samples\n",
    "bootstrap = [True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random grid\n",
    "random_grid = {\"n_estimators\": n_estimators, \"criterion\": criterion, \"max_features\": max_features, \"max_depth\": max_depth,\n",
    "               \"min_samples_split\": min_samples_split, \"bootstrap\": bootstrap}\n",
    "# Define estimator (random forest)\n",
    "rf = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random search (tries 50 different random combinations out of 800 possible ones)\n",
    "rf_random_search = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,\n",
    "                                      n_iter = 50, cv = 2, verbose = 2, n_jobs = -1)\n",
    "rf_random_search_fit = rf_random_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the best parameters found in the random search \n",
    "rf_random_search_best = rf_random_search.best_params_ \n",
    "print(rf_random_search_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Random forest with the best parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data recurring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data\n",
    "with open('Data final//data_recurring_imputed.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into dependent and independent variables\n",
    "y = data[\"Total_revenue\"]\n",
    "X = data.drop(\"Total_revenue\", axis = \"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into train and test data sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model with optimal parameters\n",
    "rf = RandomForestRegressor(n_estimators = 60, min_samples_split = 0.0005, max_features = \"sqrt\",\n",
    "                          max_depth = 15, criterion = \"squared_error\", bootstrap = False)\n",
    "\n",
    "# Train the model on training data\n",
    "rf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for the test dataset\n",
    "rf_pred = rf.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate test error (mean absolute error)\n",
    "rf_error = abs(rf_pred - test_y)\n",
    "rf_mae = np.mean(rf_error)\n",
    "# Calculate test error (mean squared error)\n",
    "rf_mse = mean_squared_error(test_y, rf_pred)\n",
    "# Calculate test error (root mean squared error)\n",
    "rf_rmse = mean_squared_error(test_y, rf_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable importance with Shapley values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 20 000 observations to calculate Shapley values\n",
    "index_sample = random.sample(range(0, len(test_X)), 20000)\n",
    "shap_X = test_X.iloc[index_sample]\n",
    "\n",
    "# Get Shapley values\n",
    "shap_values = shap.TreeExplainer(rf).shap_values(shap_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the variables by importance (box plot)\n",
    "fig = plt.figure()\n",
    "shap.summary_plot(shap_values, shap_X, plot_type=\"bar\", show = False, color = \"#9797ff\")\n",
    "plt.xlabel(\"Shapley value\")\n",
    "plt.xlim([0,160])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the variables with detail of Shapley values (beeswarm plot)\n",
    "fig = plt.figure()\n",
    "shap.summary_plot(shap_values, shap_X, show = False, cmap = \"coolwarm\")\n",
    "plt.xlabel(\"Shapley value\")\n",
    "plt.xlim([-700,3500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data\n",
    "with open('Data final//data_new_imputed.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into dependent and independent variables\n",
    "y = data[\"Total_revenue\"]\n",
    "X = data.drop(\"Total_revenue\", axis = \"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into train and test data sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model with optimal parameters\n",
    "rf = RandomForestRegressor(n_estimators = 80, min_samples_split = 0.001, max_features = \"log2\",\n",
    "                          max_depth = 20, criterion = \"squared_error\", bootstrap = True)\n",
    "# Train the model on training data\n",
    "rf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for the test dataset\n",
    "rf_pred = rf.predict(test_X)\n",
    "# Calculate test error (mean absolute error)\n",
    "rf_error = abs(rf_pred - test_y)\n",
    "rf_mae = np.mean(rf_error)\n",
    "# Calculate test error (mean squared error)\n",
    "rf_mse = mean_squared_error(test_y, rf_pred)\n",
    "# Calculate test error (root mean squared error)\n",
    "rf_rmse = mean_squared_error(test_y, rf_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable importance with Shapley values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 20 000 observations to calculate Shapley values\n",
    "index_sample = random.sample(range(0, len(test_X)), 20000)\n",
    "shap_X = test_X.iloc[index_sample]\n",
    "\n",
    "# Get Shapley values\n",
    "shap_values = shap.TreeExplainer(rf).shap_values(shap_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the variables by importance (box plot)\n",
    "fig = plt.figure()\n",
    "shap.summary_plot(shap_values, shap_X, plot_type=\"bar\", show = False, color = \"#9797ff\")\n",
    "plt.xlabel(\"Shapley value\")\n",
    "plt.xlim([0,160])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the variables with detail of Shapley values (beeswarm plot)\n",
    "fig = plt.figure()\n",
    "shap.summary_plot(shap_values, shap_X, show = False, cmap = \"coolwarm\")\n",
    "plt.xlabel(\"Shapley value\")\n",
    "plt.xlim([-700,3500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support vector regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random search with Support vector regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data recurring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data\n",
    "with open('Data final//data_recurring_imputed.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 40000 observations from data\n",
    "index_sample = random.sample(range(0, len(data)), 40000)\n",
    "data = data.iloc[index_sample,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe with features and array with outcomes (shaped in 2d array)\n",
    "y = data[\"Total revenue\"].values\n",
    "X = data.drop(\"Total revenue\", axis = \"columns\").values\n",
    "y = y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale and standardize \n",
    "StdS_X = RobustScaler()\n",
    "StdS_y = RobustScaler()\n",
    "X = StdS_X.fit_transform(X)\n",
    "y = StdS_y.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform a vector\n",
    "y_train = np.ravel(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of C values\n",
    "C_range = np.logspace(-5, 5, 10)\n",
    "print(f'The list of values for C are {C_range}')\n",
    "# List of gamma values\n",
    "gamma_range = np.logspace(-5, 5, 10)\n",
    "print(f'The list of values for gamma are {gamma_range}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the search space\n",
    "param_grid = { \n",
    "    # Regularization parameter.\n",
    "    \"C\": C_range,\n",
    "    # Kernel type\n",
    "    \"kernel\": ['rbf'],\n",
    "    # Gamma is the Kernel coefficient for rbf, poly and sigmoid\n",
    "    \"gamma\": gamma_range.tolist()+['scale', 'auto']\n",
    "    }\n",
    "# Set up score\n",
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "#scoring = ['neg_mean_absolute_error'] # uses MAE as the metric\n",
    "\n",
    "# Set up the k-fold cross-validation\n",
    "kfold = KFold(n_splits=3, shuffle=True, random_state=0) # shuffle - data is shuffled before splitting\n",
    "# random_state = 0 makes the shuffle reproducible\n",
    "# n_splits = 3 - 3-fold cross-validation\n",
    "\n",
    "# Define random search\n",
    "random_search = RandomizedSearchCV(estimator=SVR(), # chooses SVR\n",
    "                           param_distributions=param_grid, # takes our pre-defined search space for the grid search\n",
    "                           n_iter=10, # the number of parameter combinations sampled\n",
    "                           scoring= mae_scorer, # set the performance evaluation metric\n",
    "                           refit='neg_mean_absolute_error', # enables refitting the model with the best parameters on the whole training dataset.\n",
    "                           n_jobs=-1, # means parallel processing using all the processors\n",
    "                           cv=kfold, # takes the StratifiedKFold we defined\n",
    "                           verbose=0) # controls the number of messages returned by random search\n",
    "# Fit grid search\n",
    "random_result = random_search.fit(X_train, y_train)\n",
    "# Print grid search summary\n",
    "print(random_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best accuracy score for the training dataset\n",
    "print(f'The best MAE score for the training dataset is {random_result.best_score_:.4f}')\n",
    "# Print the hyperparameters for the best score\n",
    "print(f'The best hyperparameters are {random_result.best_params_}')\n",
    "# Print the best accuracy score for the testing dataset\n",
    "print(f'The MAE score for the testing dataset is {random_search.score(X_test, y_test):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_svr_model = random_search.best_estimator_\n",
    "\n",
    "# Calculate predictions for test data\n",
    "predictions = StdS_y.inverse_transform(best_svr_model.predict(X_test).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MAE\n",
    "print(f\"MAE: {mean_absolute_error(StdS_y.inverse_transform(y_test), predictions):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE\n",
    "print(f\"MAE: {mean_squared_error(StdS_y.inverse_transform(y_test), predictions):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MAE: {mean_squared_error(StdS_y.inverse_transform(y_test), predictions, squared =False):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data\n",
    "with open('Data final//data_new_imputed.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 40000 observations from data\n",
    "index_sample = random.sample(range(0, len(data)), 40000)\n",
    "data = data.iloc[index_sample,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe with features and array with outcomes (shaped in 2d array)\n",
    "y = data[\"Total_revenue\"].values\n",
    "X = data.drop(\"Total_revenue\", axis = \"columns\").values\n",
    "y = y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale and standardize \n",
    "StdS_X = RobustScaler()\n",
    "StdS_y = RobustScaler()\n",
    "X = StdS_X.fit_transform(X)\n",
    "y = StdS_y.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform a vector\n",
    "y_train = np.ravel(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the search space\n",
    "param_grid = { \n",
    "    # Regularization parameter.\n",
    "    \"C\": C_range,\n",
    "    # Kernel type\n",
    "    \"kernel\": ['rbf'],\n",
    "    # Gamma is the Kernel coefficient for rbf, poly and sigmoid.\n",
    "    \"gamma\": gamma_range.tolist()+['scale', 'auto']\n",
    "    }\n",
    "# Set up score\n",
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "#scoring = ['neg_mean_absolute_error'] # uses MAE as the metric\n",
    "\n",
    "# Set up the k-fold cross-validation\n",
    "kfold = KFold(n_splits=3, shuffle=True, random_state=0) # shuffle - data is shuffled before splitting\n",
    "# random_state = 0 makes the shuffle reproducible\n",
    "# n_splits = 3 - 3-fold cross-validation\n",
    "\n",
    "# Define random search\n",
    "random_search = RandomizedSearchCV(estimator=SVR(), # chooses SVR\n",
    "                           param_distributions=param_grid, # takes our pre-defined search space for the grid search\n",
    "                           n_iter=10, # the number of parameter combinations sampled\n",
    "                           scoring= mae_scorer, # set the performance evaluation metric\n",
    "                           refit='neg_mean_absolute_error', # enables refitting the model with the best parameters on the whole training dataset.\n",
    "                           n_jobs=-1, # means parallel processing using all the processors\n",
    "                           cv=kfold, # takes the StratifiedKFold we defined\n",
    "                           verbose=0) # controls the number of messages returned by random search\n",
    "# Fit grid search\n",
    "random_result = random_search.fit(X_train, y_train)\n",
    "# Print grid search summary\n",
    "print(random_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best accuracy score for the training dataset\n",
    "print(f'The best MAE score for the training dataset is {random_result.best_score_:.4f}')\n",
    "# Print the hyperparameters for the best score\n",
    "print(f'The best hyperparameters are {random_result.best_params_}')\n",
    "# Print the best accuracy score for the testing dataset\n",
    "print(f'The MAE score for the testing dataset is {random_search.score(X_test, y_test):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_svr_model = random_search.best_estimator_\n",
    "# Calculate predictions for test data\n",
    "predictions = StdS_y.inverse_transform(best_svr_model.predict(X_test).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MAE\n",
    "print(f\"MAE: {mean_absolute_error(StdS_y.inverse_transform(y_test), predictions):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE\n",
    "print(f\"MAE: {mean_squared_error(StdS_y.inverse_transform(y_test), predictions):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MAE: {mean_squared_error(StdS_y.inverse_transform(y_test), predictions, squared =False):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Support Vector regression with the best parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data recurring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data\n",
    "with open('Data final//data_recurring_imputed.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 100000 observations from data\n",
    "index_sample = random.sample(range(0, len(data)), 100000)\n",
    "data = data.iloc[index_sample,]\n",
    "# Dataframe with features and array with outcomes (shaped in 2d array)\n",
    "y = data[\"Total revenue\"].values\n",
    "X = data.drop(\"Total revenue\", axis = \"columns\").values\n",
    "y = y.reshape(-1,1)\n",
    "# Rescale and standardize \n",
    "StdS_X = RobustScaler()\n",
    "StdS_y = RobustScaler()\n",
    "X = StdS_X.fit_transform(X)\n",
    "y = StdS_y.fit_transform(y)\n",
    "# Sample splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "# Transform a vector\n",
    "y_train = np.ravel(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model object\n",
    "regressor = SVR(kernel = 'rbf', gamma = 0.00001, C = 7742.63)\n",
    "# Fit the model on the data\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions \n",
    "predictions = StdS_y.inverse_transform(regressor.predict(X_test).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MAE\n",
    "print(f\"MAE: {mean_absolute_error(StdS_y.inverse_transform(y_test), predictions):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MAE\n",
    "print(f\"MAE standardized after the split: {mean_squared_error(StdS_y.inverse_transform(y_test), predictions):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MAE\n",
    "print(f\"MAE with a robust scaler: {mean_squared_error(StdS_y.inverse_transform(y_test), predictions, squared=False):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data\n",
    "with open('Data final//data_new_imputed.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 100000 observations from data\n",
    "index_sample = random.sample(range(0, len(data)), 100000)\n",
    "data = data.iloc[index_sample,]\n",
    "# Dataframe with features and array with outcomes (shaped in 2d array)\n",
    "y = data[\"Total revenue\"].values\n",
    "X = data.drop(\"Total revenue\", axis = \"columns\").values\n",
    "y = y.reshape(-1,1)\n",
    "# Rescale and standardize \n",
    "StdS_X = RobustScaler()\n",
    "StdS_y = RobustScaler()\n",
    "X = StdS_X.fit_transform(X)\n",
    "y = StdS_y.fit_transform(y)\n",
    "# Sample splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "# Transform a vector\n",
    "y_train = np.ravel(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model object\n",
    "regressor = SVR(kernel = 'rbf', gamma = 0.00013, C = 599.48)\n",
    "# Fit the model on the data\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions \n",
    "predictions = StdS_y.inverse_transform(regressor.predict(X_test).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MAE\n",
    "print(f\"MAE: {mean_absolute_error(StdS_y.inverse_transform(y_test), predictions):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MAE\n",
    "print(f\"MAE standardized after the split: {mean_squared_error(StdS_y.inverse_transform(y_test), predictions):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MAE\n",
    "print(f\"MAE with a robust scaler: {mean_squared_error(StdS_y.inverse_transform(y_test), predictions, squared=False):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
