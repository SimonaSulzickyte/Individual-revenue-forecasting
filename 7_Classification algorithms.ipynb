{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5908eee7",
   "metadata": {},
   "source": [
    "# Classification algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb1f733",
   "metadata": {},
   "source": [
    "Compare Random Forest and XGBoost for classifying customers into clusters. For new customers, XGBoost with SMOTE+Tomek is also tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b689a897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pickle\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from xgboost import XGBRegressor, XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dda790",
   "metadata": {},
   "source": [
    "## Recurring customers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0ffde3",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810866d0",
   "metadata": {},
   "source": [
    "Random search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa6abea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "with open('Data clustering\\\\data_recurring_clusters.pkl', 'rb') as file:\n",
    "    data_recurring_clusters = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b1bad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 100 000 rows to perform a random search\n",
    "index_sample = random.sample(range(0, len(data_recurring_clusters)), 100000)\n",
    "data_recurring_clusters = data_recurring_clusters.iloc[index_sample,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7aeda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into dependent and independent variables\n",
    "y = data_recurring_clusters[\"Cluster\"]\n",
    "X = data_recurring_clusters.drop([\"Total revenue\", \"Cluster\"], axis = \"columns\")\n",
    "# Divide data into train and test data sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb76154d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define search range for random search for...\n",
    "# number of decision trees to use\n",
    "n_estimators = [40, 50, 60, 70, 80] \n",
    "# loss function\n",
    "criterion = [\"gini\", \"entropy\"]\n",
    "# maximum number of features to consider at every split\n",
    "max_features = [\"log2\", \"sqrt\"]\n",
    "# maximum number of levels in each tree\n",
    "max_depth = [3, 5, 10, 15, 20]\n",
    "# minimum number of observations to split a node \n",
    "min_samples_split = [0.0005, 0.001, 0.0025, 0.005] \n",
    "# use bootstrap samples\n",
    "bootstrap = [True]\n",
    "\n",
    "# Create random grid\n",
    "random_grid = {\"n_estimators\": n_estimators, \"criterion\": criterion, \"max_features\": max_features, \"max_depth\": max_depth,\n",
    "               \"min_samples_split\": min_samples_split, \"bootstrap\": bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426c9d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model\n",
    "rf = RandomForestClassifier()\n",
    "# Random search (tries 50 different random combinations out of 400 possible ones)\n",
    "random_search = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,\n",
    "                                      n_iter = 50, cv = 2, verbose = 2, n_jobs = -1,\n",
    "                                      scoring = \"roc_auc_ovr_weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb68b667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a random search\n",
    "random_search.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b52b87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the best parameters\n",
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379a6e88",
   "metadata": {},
   "source": [
    "Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31400f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "with open('Data clustering\\\\data_recurring_clusters.pkl', 'rb') as file:\n",
    "    data_recurring_clusters = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c3da19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into dependent and independent variables\n",
    "y = data_recurring_clusters[\"Cluster\"]\n",
    "X = data_recurring_clusters.drop([\"Total revenue\", \"Cluster\"], axis = \"columns\")\n",
    "# Divide data into train and test data sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac2da3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model with the best parameters \n",
    "rf = RandomForestClassifier(n_estimators = 80, min_samples_split = 0.0005, max_features = \"log2\",\n",
    "                                     max_depth = 20, criterion = \"entropy\", bootstrap = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eae4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "rf.fit(train_X, train_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b84a485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "pred = rf.predict(test_X)\n",
    "pred_prob = rf.predict_proba(test_X)\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(test_y, pred)\n",
    "# Calculate weighted roc_auc score\n",
    "auc = roc_auc_score(test_y, pred_prob, multi_class = \"ovr\", average = \"weighted\") \n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"AUC: {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18c3f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix \n",
    "plt.figure(figsize=(100,100))\n",
    "cm = confusion_matrix(test_y, pred)\n",
    "cm_display = ConfusionMatrixDisplay(cm, display_labels = [\"Low\", \"Medium\", \"High\"])\n",
    "fig, ax = plt.subplots(figsize=(8,6)) \n",
    "cm_display.plot(cmap = \"Blues\", values_format = \"d\", colorbar = False, ax = ax) \n",
    "for text in cm_display.text_.ravel():\n",
    "    text.set_fontsize(14)\n",
    "ax.set_xlabel(\"Predicted label\", fontsize=14)\n",
    "ax.set_ylabel(\"True label\", fontsize=14)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.savefig(\"CM_RF_Recurring.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61caec0",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42d88b9",
   "metadata": {},
   "source": [
    "Random search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449cd15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "with open('Data clustering\\\\data_recurring_clusters.pkl', 'rb') as file:\n",
    "    data_recurring_clusters = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599218b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 1 000 000 rows to perform a random search\n",
    "index_sample = random.sample(range(0, len(data_recurring_clusters)), 1000000)\n",
    "data_recurring_clusters = data_recurring_clusters.iloc[index_sample,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42617829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into dependent and independent variables\n",
    "y = data_recurring_clusters[\"Cluster\"]\n",
    "X = data_recurring_clusters.drop([\"Total revenue\", \"Cluster\"], axis = \"columns\")\n",
    "# Divide data into train and test data sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2d94a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters that will be checked for a random search\n",
    "params = {\n",
    "    'n_estimators':[70, 100, 200],\n",
    "    'min_child_weight':[1, 2, 3], \n",
    "    'gamma':[i/10.0 for i in range(0,3)],  \n",
    "    'subsample':[i/10.0 for i in range(6,11)],\n",
    "    'colsample_bytree':[i/10.0 for i in range(6,11)], \n",
    "    'max_depth': [2,3,4,5,6,7],\n",
    "    'objective': ['multi:softmax'],\n",
    "    'booster': ['gbtree'],\n",
    "    'eta': [i/10.0 for i in range(3,6)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f23bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model\n",
    "cla = XGBClassifier(nthread=-1)\n",
    "# Set the number of iterations for a random search\n",
    "n_iter_search = 50\n",
    "# Define a random search\n",
    "random_search = RandomizedSearchCV(cla, param_distributions=params,\n",
    "                                   n_iter=n_iter_search, cv=5, scoring= 'roc_auc_ovr_weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c6bb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a random search\n",
    "random_search.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b13fafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the best parameters\n",
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e466217",
   "metadata": {},
   "source": [
    "Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe99177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "with open('Data clustering\\\\data_recurring_clusters.pkl', 'rb') as file:\n",
    "    data_recurring_clusters = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1617d52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into dependent and independent variables\n",
    "y = data_recurring_clusters[\"Cluster\"]\n",
    "X = data_recurring_clusters.drop([\"Total revenue\", \"Cluster\"], axis = \"columns\")\n",
    "# Divide data into train and test data sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725e28c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model with the best parameters \n",
    "model_xgb = XGBClassifier(subsample = 0.9, objective = 'multi:softmax', n_estimators = 200, min_child_weight = 3, \n",
    "                    max_depth = 6, gamma = 0.2, eta = 0.3, colsample_bytree = 0.6, booster = 'gbtree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b21490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model_xgb.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0154afa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "pred = model_xgb.predict(test_X)\n",
    "pred_prob = model_xgb.predict_proba(test_X)\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(test_y, pred)\n",
    "# Calculate weighted roc_auc score\n",
    "auc = roc_auc_score(test_y, pred_prob, multi_class = \"ovr\", average = \"weighted\") \n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"AUC: {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921c7306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix \n",
    "plt.figure(figsize=(100,100))\n",
    "cm = confusion_matrix(test_y, pred)\n",
    "cm_display = ConfusionMatrixDisplay(cm, display_labels = [\"Low\", \"Medium\", \"High\"])\n",
    "fig, ax = plt.subplots(figsize=(8,6)) \n",
    "cm_display.plot(cmap = \"Blues\", values_format = \"d\", colorbar = False, ax = ax) \n",
    "for text in cm_display.text_.ravel():\n",
    "    text.set_fontsize(14)\n",
    "ax.set_xlabel(\"Predicted label\", fontsize=14)\n",
    "ax.set_ylabel(\"True label\", fontsize=14)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.savefig(\"CM_XGBoost_Recurring.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e996f14",
   "metadata": {},
   "source": [
    "## New customers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb96f83",
   "metadata": {},
   "source": [
    "### Random forest "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1d210f",
   "metadata": {},
   "source": [
    "Random search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ca7fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "with open('Data clustering\\\\data_new_clusters.pkl', 'rb') as file:\n",
    "    data_new_clusters = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4276461a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 100 000 rows to perform a random search\n",
    "index_sample = random.sample(range(0, len(data_new_clusters)), 100000)\n",
    "data_new_clusters = data_new_clusters.iloc[index_sample,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3297cb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into dependent and independent variables\n",
    "y = data_new_clusters[\"Cluster\"]\n",
    "X = data_new_clusters.drop([\"Total revenue\", \"Cluster\"], axis = \"columns\")\n",
    "# Divide data into train and test data sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65acf71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define search range for random search for...\n",
    "# number of decision trees to use\n",
    "n_estimators = [40, 50, 60, 70, 80] \n",
    "# loss function\n",
    "criterion = [\"gini\", \"entropy\"]\n",
    "# maximum number of features to consider at every split\n",
    "max_features = [\"log2\", \"sqrt\"]\n",
    "# maximum number of levels in each tree\n",
    "max_depth = [3, 5, 10, 15, 20]\n",
    "# minimum number of observations to split a node \n",
    "min_samples_split = [0.0005, 0.001, 0.0025, 0.005] \n",
    "# use bootstrap samples\n",
    "bootstrap = [True]\n",
    "\n",
    "# Create random grid\n",
    "random_grid = {\"n_estimators\": n_estimators, \"criterion\": criterion, \"max_features\": max_features, \"max_depth\": max_depth,\n",
    "               \"min_samples_split\": min_samples_split, \"bootstrap\": bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5714c342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model\n",
    "rf = RandomForestClassifier()\n",
    "# Random search (tries 50 different random combinations out of 400 possible ones)\n",
    "random_search = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,\n",
    "                                      n_iter = 50, cv = 2, verbose = 2, n_jobs = -1,\n",
    "                                      scoring = \"roc_auc_ovr_weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef2d5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a random search\n",
    "random_search.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65443331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the best parameters\n",
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88c9b85",
   "metadata": {},
   "source": [
    "Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93037a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "with open('Data clustering\\\\data_new_clusters.pkl', 'rb') as file:\n",
    "    data_new_clusters = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496a1af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into dependent and independent variables\n",
    "y = data_new_clusters[\"Cluster\"]\n",
    "X = data_new_clusters.drop([\"Total_revenue\", \"Cluster\"], axis = \"columns\")\n",
    "# Divide data into train and test data sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11d1754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model with the best parameters \n",
    "rf = RandomForestClassifier(n_estimators = 80, min_samples_split = 0.0005, max_features = \"sqrt\",\n",
    "                                     max_depth = 10, criterion = \"entropy\", bootstrap = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f8d1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on training data\n",
    "rf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83efc9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "pred = rf.predict(test_X)\n",
    "pred_prob = rf.predict_proba(test_X)\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(test_y, pred)\n",
    "# Calculate weighted roc_auc score\n",
    "auc = roc_auc_score(test_y, pred_prob, multi_class = \"ovr\", average = \"weighted\") \n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"AUC: {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24dc446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix \n",
    "plt.figure(figsize=(100,100))\n",
    "cm = confusion_matrix(test_y, pred)\n",
    "cm_display = ConfusionMatrixDisplay(cm, display_labels = [\"Low\", \"Medium\", \"High\"])\n",
    "fig, ax = plt.subplots(figsize=(8,6)) \n",
    "cm_display.plot(cmap = \"Blues\", values_format = \"d\", colorbar = False, ax = ax) \n",
    "for text in cm_display.text_.ravel():\n",
    "    text.set_fontsize(14)\n",
    "ax.set_xlabel(\"Predicted label\", fontsize=14)\n",
    "ax.set_ylabel(\"True label\", fontsize=14)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.savefig(\"CM_RF_New.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1bb1cb",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607f5193",
   "metadata": {},
   "source": [
    "Random search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d19f602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "with open('Data clustering\\\\data_new_clusters.pkl', 'rb') as file:\n",
    "    data_new_clusters = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c21803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into dependent and independent variables\n",
    "y = data_new_clusters[\"Cluster\"]\n",
    "X = data_new_clusters.drop([\"Total revenue\", \"Cluster\"], axis = \"columns\")\n",
    "# Divide data into train and test data sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e357a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters that will be checked for a random search\n",
    "params = {\n",
    "    'n_estimators':[70, 100, 200],\n",
    "    'min_child_weight':[1, 2, 3], \n",
    "    'gamma':[i/10.0 for i in range(0,3)],  \n",
    "    'subsample':[i/10.0 for i in range(6,11)],\n",
    "    'colsample_bytree':[i/10.0 for i in range(6,11)], \n",
    "    'max_depth': [2,3,4,6,7],\n",
    "    'objective': ['multi:softmax'],\n",
    "    'booster': ['gbtree', 'gblinear', 'dart'],\n",
    "    'eta': [i/10.0 for i in range(3,6)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f4b0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model\n",
    "cla = XGBClassifier(nthread=-1)\n",
    "# Set the number of iterations for a random search\n",
    "n_iter_search = 50\n",
    "# Define a random search\n",
    "random_search = RandomizedSearchCV(cla, param_distributions=params,\n",
    "                                   n_iter=n_iter_search, cv=5, scoring= 'roc_auc_ovr_weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a334a995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a random search\n",
    "random_search.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a20e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the best parameters\n",
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4880d457",
   "metadata": {},
   "source": [
    "Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4401732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "with open('Data clustering\\\\data_new_clusters.pkl', 'rb') as file:\n",
    "    data_new_clusters = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c15fa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into dependent and independent variables\n",
    "y = data_new_clusters[\"Cluster\"]\n",
    "X = data_new_clusters.drop([\"Total revenue\", \"Cluster\"], axis = \"columns\")\n",
    "# Divide data into train and test data sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfa9ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model with the best parameters \n",
    "model_xgb = XGBClassifier(subsample = 1, objective = 'multi:softmax', n_estimators = 200, min_child_weight = 2, \n",
    "                    max_depth = 5, gamma = 0.2, eta = 0.5, colsample_bytree = 0.9, booster = 'gbtree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403c2f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model_xgb.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b70ea32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "pred = model_xgb.predict(test_X)\n",
    "pred_prob = model_xgb.predict_proba(test_X)\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(test_y, pred)\n",
    "# Calculate weighted roc_auc score\n",
    "auc = roc_auc_score(test_y, pred_prob, multi_class = \"ovr\", average = \"weighted\") \n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"AUC: {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12107ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix \n",
    "plt.figure(figsize=(100,100))\n",
    "cm = confusion_matrix(test_y, pred)\n",
    "cm_display = ConfusionMatrixDisplay(cm, display_labels = [\"Low\", \"Medium\", \"High\"])\n",
    "fig, ax = plt.subplots(figsize=(8,6)) \n",
    "cm_display.plot(cmap = \"Blues\", values_format = \"d\", colorbar = False, ax = ax) \n",
    "for text in cm_display.text_.ravel():\n",
    "    text.set_fontsize(14)\n",
    "ax.set_xlabel(\"Predicted label\", fontsize=14)\n",
    "ax.set_ylabel(\"True label\", fontsize=14)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.savefig(\"CM_XGBoost_New.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda8c1a7",
   "metadata": {},
   "source": [
    "### XGBoost + SMOTE-Tomek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90c0ff1",
   "metadata": {},
   "source": [
    "SMOTE-Tomek resampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30a8fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "with open('Data clustering\\\\data_new_clusters.pkl', 'rb') as file:\n",
    "    data_new_clusters = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61d1bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into dependent and independent variables\n",
    "y = data_new_clusters[\"Cluster\"]\n",
    "X = data_new_clusters.drop([\"Total revenue\", \"Cluster\"], axis = \"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572a87cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variables \n",
    "cat_vars = X.columns.isin([\"Insurance\", \"Co-applicant\", \"Gender\", \"Quarter_2\", \"Quarter_3\", \"Quarter_4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08d79ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into train and test data sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54ce52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample with Smote-Tomek links\n",
    "resample = SMOTETomek(smote = SMOTENC(categorical_features = cat_vars, sampling_strategy = \"not majority\", k_neighbors = 5),  \n",
    "                      tomek = TomekLinks(sampling_strategy = \"all\"), \n",
    "                      random_state = 3)\n",
    "resampled_data = resample.fit_resample(train_X, train_y)\n",
    "train_X_resampled = resampled_data[0]\n",
    "train_y_resampled = resampled_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ba81e8",
   "metadata": {},
   "source": [
    "Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57049fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "with open('Data clustering\\\\data_new_clusters.pkl', 'rb') as file:\n",
    "    data_new_clusters = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fa4655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model with the best parameters \n",
    "model_xgb = XGBClassifier(subsample = 1, objective = 'multi:softmax', n_estimators = 200, min_child_weight = 2, \n",
    "                    max_depth = 5, gamma = 0.2, eta = 0.5, colsample_bytree = 0.9, booster = 'gbtree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb09dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "pred = model_xgb.predict(test_X)\n",
    "pred_prob = model_xgb.predict_proba(test_X)\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(test_y, pred)\n",
    "# Calculate weighted roc_auc score\n",
    "auc = roc_auc_score(test_y, pred_prob, multi_class = \"ovr\", average = \"weighted\") \n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"AUC: {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0a004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix \n",
    "plt.figure(figsize=(100,100))\n",
    "cm = confusion_matrix(test_y, pred)\n",
    "cm_display = ConfusionMatrixDisplay(cm, display_labels = [\"Low\", \"Medium\", \"High\"])\n",
    "fig, ax = plt.subplots(figsize=(8,6)) \n",
    "cm_display.plot(cmap = \"Blues\", values_format = \"d\", colorbar = False, ax = ax) \n",
    "for text in cm_display.text_.ravel():\n",
    "    text.set_fontsize(14)\n",
    "ax.set_xlabel(\"Predicted label\", fontsize=14)\n",
    "ax.set_ylabel(\"True label\", fontsize=14)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.savefig(\"CM_XGBoost_ST_New.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
